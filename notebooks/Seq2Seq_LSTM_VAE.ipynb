{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Setup"
      ],
      "metadata": {
        "id": "18TljAFXol5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSz37TYIoj6B"
      },
      "outputs": [],
      "source": [
        "# Setup - Run this first\n",
        "!pip install torch matplotlib seaborn pandas numpy scikit-learn tqdm -q\n",
        "\n",
        "import os, warnings, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Upload Data"
      ],
      "metadata": {
        "id": "YFf4ujioopqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def check_data_files():\n",
        "    \"\"\"Check if required files already exist\"\"\"\n",
        "    required_files = ['studentInfo.csv', 'studentVle.csv', 'studentAssessment.csv']\n",
        "    existing = []\n",
        "    missing = []\n",
        "\n",
        "    for file in required_files:\n",
        "        if os.path.exists(f'data/raw/{file}'):\n",
        "            existing.append(file)\n",
        "        else:\n",
        "            missing.append(file)\n",
        "\n",
        "    if existing:\n",
        "        print(f\"‚úÖ Found existing files: {existing}\")\n",
        "    if missing:\n",
        "        print(f\"‚ùå Missing files: {missing}\")\n",
        "\n",
        "    return len(missing) == 0\n",
        "\n",
        "def download_oulad_dataset():\n",
        "    \"\"\"Attempt to automatically download OULAD dataset\"\"\"\n",
        "    print(\"üîÑ Attempting to download OULAD dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Download URL (Kaggle public dataset)\n",
        "        url = \"https://www.kaggle.com/api/v1/datasets/download/anlgrbz/student-demographics-online-education-dataoulad\"\n",
        "        zip_path = \"oulad_dataset.zip\"\n",
        "\n",
        "        print(\"üì• Downloading dataset (this may take a few minutes)...\")\n",
        "\n",
        "        # Try using curl first (more reliable for large files)\n",
        "        try:\n",
        "            import subprocess\n",
        "            result = subprocess.run([\n",
        "                'curl', '-L', '-o', zip_path, url\n",
        "            ], capture_output=True, text=True, timeout=300)  # 5 minute timeout\n",
        "\n",
        "            if result.returncode != 0:\n",
        "                raise Exception(f\"Curl failed: {result.stderr}\")\n",
        "\n",
        "        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:\n",
        "            print(f\"‚ö†Ô∏è Curl method failed: {e}\")\n",
        "            print(\"üì• Trying alternative download method...\")\n",
        "\n",
        "            # Fallback to urllib\n",
        "            urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "        # Check if download was successful\n",
        "        if not os.path.exists(zip_path) or os.path.getsize(zip_path) < 1000:  # Less than 1KB indicates failure\n",
        "            raise Exception(\"Downloaded file is too small or doesn't exist\")\n",
        "\n",
        "        file_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "        print(f\"‚úÖ Download complete: {file_size_mb:.1f} MB\")\n",
        "\n",
        "        # Extract the zip file\n",
        "        print(\"üìÇ Extracting files...\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # List all files in the zip\n",
        "            all_files = zip_ref.namelist()\n",
        "            print(f\"Found {len(all_files)} files in archive\")\n",
        "\n",
        "            required_files = ['studentInfo.csv', 'studentVle.csv', 'studentAssessment.csv']\n",
        "            extracted_files = []\n",
        "\n",
        "            # Extract required files\n",
        "            for file_path in all_files:\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                if filename in required_files:\n",
        "                    # Read file data from zip\n",
        "                    with zip_ref.open(file_path) as source:\n",
        "                        # Write to data/raw/\n",
        "                        target_path = f'data/raw/{filename}'\n",
        "                        with open(target_path, 'wb') as target:\n",
        "                            target.write(source.read())\n",
        "\n",
        "                    extracted_files.append(filename)\n",
        "                    file_size = os.path.getsize(target_path) / (1024 * 1024)\n",
        "                    print(f\"  ‚úÖ Extracted {filename}: {file_size:.1f} MB\")\n",
        "\n",
        "            # Check if all required files were found\n",
        "            missing_files = [f for f in required_files if f not in extracted_files]\n",
        "            if missing_files:\n",
        "                raise Exception(f\"Required files not found in archive: {missing_files}\")\n",
        "\n",
        "        # Clean up zip file\n",
        "        os.remove(zip_path)\n",
        "        print(\"üóëÔ∏è Cleaned up temporary files\")\n",
        "\n",
        "        print(\"üéâ Automatic download completed successfully!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Automatic download failed: {str(e)}\")\n",
        "\n",
        "        # Clean up failed download\n",
        "        if os.path.exists(zip_path):\n",
        "            try:\n",
        "                os.remove(zip_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return False\n",
        "\n",
        "def manual_upload():\n",
        "    \"\"\"Fallback to manual file upload\"\"\"\n",
        "    print(\"\\nüì§ Please upload the OULAD CSV files manually:\")\n",
        "    print(\"   Required files: studentInfo.csv, studentVle.csv, studentAssessment.csv\")\n",
        "    print(\"   Download from: https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad\")\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"\\n‚¨áÔ∏è Click 'Choose Files' below and select all 3 CSV files:\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        moved_files = []\n",
        "        for filename in uploaded.keys():\n",
        "            if filename.endswith('.csv'):\n",
        "                # Move to data/raw/\n",
        "                target_path = f'data/raw/{filename}'\n",
        "                os.rename(filename, target_path)\n",
        "\n",
        "                file_size = os.path.getsize(target_path) / (1024 * 1024)\n",
        "                print(f\"‚úÖ Uploaded {filename}: {file_size:.1f} MB\")\n",
        "                moved_files.append(filename)\n",
        "\n",
        "        if len(moved_files) == 3:\n",
        "            print(\"üéâ Manual upload completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Only {len(moved_files)} files uploaded. Need 3 files.\")\n",
        "            return False\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Manual upload not available (not running in Colab)\")\n",
        "        print(\"Please manually place CSV files in data/raw/ directory\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Manual upload failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main execution\n",
        "print(\"üîç Checking for existing data files...\")\n",
        "\n",
        "if check_data_files():\n",
        "    print(\"‚úÖ All required data files already exist! Skipping download.\")\n",
        "else:\n",
        "    print(\"\\nüöÄ Starting data acquisition process...\")\n",
        "\n",
        "    # Try automatic download first\n",
        "    download_success = download_oulad_dataset()\n",
        "\n",
        "    if not download_success:\n",
        "        print(\"\\nüîÑ Automatic download failed. Trying manual upload...\")\n",
        "        upload_success = manual_upload()\n",
        "\n",
        "        if not upload_success:\n",
        "            print(\"\\n‚ùå Both automatic and manual methods failed.\")\n",
        "            print(\"üìã Manual steps:\")\n",
        "            print(\"1. Go to: https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad\")\n",
        "            print(\"2. Download the dataset\")\n",
        "            print(\"3. Extract and upload the 3 CSV files using the file browser on the left\")\n",
        "            print(\"4. Place them in the data/raw/ folder\")\n",
        "\n",
        "    # Final check\n",
        "    print(\"\\nüîç Final verification...\")\n",
        "    if check_data_files():\n",
        "        print(\"üéâ All data files are ready!\")\n",
        "\n",
        "        # Show file info\n",
        "        required_files = ['studentInfo.csv', 'studentVle.csv', 'studentAssessment.csv']\n",
        "        print(\"\\nüìä Dataset info:\")\n",
        "        total_size = 0\n",
        "        for filename in required_files:\n",
        "            filepath = f'data/raw/{filename}'\n",
        "            if os.path.exists(filepath):\n",
        "                size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "                total_size += size_mb\n",
        "                print(f\"   üìÑ {filename}: {size_mb:.1f} MB\")\n",
        "\n",
        "        print(f\"   üíæ Total size: {total_size:.1f} MB\")\n",
        "        print(\"\\n‚úÖ Ready to proceed to next cell!\")\n",
        "    else:\n",
        "        print(\"‚ùå Data files still missing. Please upload manually.\")\n"
      ],
      "metadata": {
        "id": "7X3ImUTWor5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Load and Process Data"
      ],
      "metadata": {
        "id": "_WzcfA2CotXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_oulad():\n",
        "    # Load CSV files\n",
        "    print(\"Loading OULAD data...\")\n",
        "    student_info = pd.read_csv('data/raw/studentInfo.csv')\n",
        "    student_vle = pd.read_csv('data/raw/studentVle.csv')\n",
        "    student_assessment = pd.read_csv('data/raw/studentAssessment.csv')\n",
        "\n",
        "    print(f\"Loaded: Info={student_info.shape}, VLE={student_vle.shape}, Assessment={student_assessment.shape}\")\n",
        "    print(f\"VLE columns: {list(student_vle.columns)}\")\n",
        "    print(f\"Assessment columns: {list(student_assessment.columns)}\")\n",
        "\n",
        "    # Handle date columns flexibly\n",
        "    vle_date_col = 'date' if 'date' in student_vle.columns else student_vle.columns[1]\n",
        "    student_vle['week'] = student_vle[vle_date_col] // 7\n",
        "\n",
        "    # Handle assessment dates\n",
        "    if 'date_submitted' in student_assessment.columns:\n",
        "        student_assessment['week'] = student_assessment['date_submitted'] // 7\n",
        "    elif 'date' in student_assessment.columns:\n",
        "        student_assessment['week'] = student_assessment['date'] // 7\n",
        "    else:\n",
        "        student_assessment['week'] = 0\n",
        "        print(\"Warning: No date column found in assessments\")\n",
        "\n",
        "    # Handle click columns\n",
        "    click_col = 'sum_click' if 'sum_click' in student_vle.columns else 'clicks'\n",
        "\n",
        "    # Aggregate clicks per week\n",
        "    clicks_df = student_vle.groupby(['id_student', 'week'])[click_col].sum().reset_index()\n",
        "    clicks_df.columns = ['id_student', 'week', 'clicks']\n",
        "\n",
        "    # Handle assessment scores\n",
        "    if 'score' in student_assessment.columns:\n",
        "        # Convert text scores to numeric if needed\n",
        "        if student_assessment['score'].dtype == 'object':\n",
        "            score_map = {'Pass': 70, 'Fail': 30, 'Distinction': 85, 'Withdrawn': 0}\n",
        "            student_assessment['score'] = student_assessment['score'].map(score_map).fillna(0)\n",
        "\n",
        "        submit_df = student_assessment.groupby(['id_student', 'week'])['score'].agg(['count', 'mean']).reset_index()\n",
        "        submit_df.columns = ['id_student', 'week', 'submit_cnt', 'avg_score']\n",
        "    else:\n",
        "        submit_df = pd.DataFrame(columns=['id_student', 'week', 'submit_cnt', 'avg_score'])\n",
        "\n",
        "    # Create complete time grid for all students\n",
        "    all_students = student_info['id_student'].unique()\n",
        "    all_weeks = []\n",
        "    for student in all_students:\n",
        "        for week in range(30):  # 30 weeks max\n",
        "            all_weeks.append({'id_student': student, 'week': week})\n",
        "\n",
        "    df = pd.DataFrame(all_weeks)\n",
        "\n",
        "    # Merge features\n",
        "    df = df.merge(clicks_df, on=['id_student', 'week'], how='left')\n",
        "    if not submit_df.empty:\n",
        "        df = df.merge(submit_df, on=['id_student', 'week'], how='left')\n",
        "    else:\n",
        "        df['submit_cnt'] = 0\n",
        "        df['avg_score'] = 0\n",
        "\n",
        "    # Fill missing values\n",
        "    df.fillna(0, inplace=True)\n",
        "    df = df.sort_values(['id_student', 'week']).reset_index(drop=True)\n",
        "\n",
        "    # Create derived features\n",
        "    df['has_submit'] = (df['submit_cnt'] > 0).astype(int)\n",
        "    df['avg_score_sofar'] = df.groupby('id_student')['avg_score'].expanding().mean().values\n",
        "    df['clicks_diff1'] = df.groupby('id_student')['clicks'].diff().fillna(0)\n",
        "\n",
        "    print(f\"Created weekly features: {df.shape}\")\n",
        "    print(f\"Average clicks per week: {df['clicks'].mean():.1f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "weekly_df = load_and_process_oulad()\n"
      ],
      "metadata": {
        "id": "d9jkPAX9ourS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Create Sequences"
      ],
      "metadata": {
        "id": "XcsjkN_AowQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, input_weeks=4, output_weeks=2):\n",
        "    \"\"\"Create input-output sequences\"\"\"\n",
        "    feature_cols = ['clicks', 'has_submit', 'avg_score_sofar', 'clicks_diff1']\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    student_list = []\n",
        "\n",
        "    print(\"Creating sequences...\")\n",
        "    for student_id, group in tqdm(df.groupby('id_student')):\n",
        "        group = group.sort_values('week').reset_index(drop=True)\n",
        "\n",
        "        if len(group) < input_weeks + output_weeks:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(group) - input_weeks - output_weeks + 1):\n",
        "            # Input: 4 weeks of 4 features\n",
        "            X_window = group.iloc[i:i+input_weeks][feature_cols].values\n",
        "            # Output: 2 weeks of clicks only\n",
        "            y_window = group.iloc[i+input_weeks:i+input_weeks+output_weeks][['clicks']].values\n",
        "\n",
        "            X_list.append(X_window)\n",
        "            y_list.append(y_window)\n",
        "            student_list.append(student_id)\n",
        "\n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=np.float32)\n",
        "    student_ids = np.array(student_list)\n",
        "\n",
        "    print(f\"Created {len(X)} sequences\")\n",
        "    print(f\"Input shape: {X.shape}\")\n",
        "    print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "    return X, y, student_ids\n",
        "\n",
        "def split_and_normalize(X, y, student_ids):\n",
        "    \"\"\"Split by students and normalize\"\"\"\n",
        "    print(\"Splitting data by student ID...\")\n",
        "\n",
        "    # Split by student to avoid data leakage\n",
        "    unique_students = np.unique(student_ids)\n",
        "    train_students, test_students = train_test_split(unique_students, test_size=0.2, random_state=42)\n",
        "    train_students, val_students = train_test_split(train_students, test_size=0.1, random_state=42)\n",
        "\n",
        "    train_mask = np.isin(student_ids, train_students)\n",
        "    val_mask = np.isin(student_ids, val_students)\n",
        "    test_mask = np.isin(student_ids, test_students)\n",
        "\n",
        "    X_train, y_train = X[train_mask], y[train_mask]\n",
        "    X_val, y_val = X[val_mask], y[val_mask]\n",
        "    X_test, y_test = X[test_mask], y[test_mask]\n",
        "\n",
        "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    # Normalize using training statistics only\n",
        "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
        "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std() + 1e-8\n",
        "\n",
        "    X_train = (X_train - X_mean) / X_std\n",
        "    X_val = (X_val - X_mean) / X_std\n",
        "    X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "    y_train = (y_train - y_mean) / y_std\n",
        "    y_val = (y_val - y_mean) / y_std\n",
        "    y_test = (y_test - y_mean) / y_std\n",
        "\n",
        "    print(\"Data normalized successfully\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train, 'y_train': y_train,\n",
        "        'X_val': X_val, 'y_val': y_val,\n",
        "        'X_test': X_test, 'y_test': y_test,\n",
        "        'norm_stats': {'y_mean': y_mean, 'y_std': y_std}\n",
        "    }\n",
        "\n",
        "# Create sequences and split data\n",
        "X, y, student_ids = create_sequences(weekly_df)\n",
        "data = split_and_normalize(X, y, student_ids)\n"
      ],
      "metadata": {
        "id": "FylkHcPxox_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Define Models"
      ],
      "metadata": {
        "id": "L6oFAR4yozpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningBehaviorDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(output_size, hidden_size, batch_first=True)\n",
        "        self.output_proj = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, tgt_len):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Encode\n",
        "        _, (hidden, cell) = self.encoder(x)\n",
        "\n",
        "        # Decode\n",
        "        decoder_input = torch.zeros(batch_size, 1, self.output_size, device=x.device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(tgt_len):\n",
        "            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
        "            output = self.output_proj(decoder_output)\n",
        "            outputs.append(output)\n",
        "            decoder_input = output\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "class Seq2SeqVAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_dim, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.mu_proj = nn.Linear(hidden_size, latent_dim)\n",
        "        self.logvar_proj = nn.Linear(hidden_size, latent_dim)\n",
        "\n",
        "        self.decoder = nn.LSTM(output_size + latent_dim, hidden_size, batch_first=True)\n",
        "        self.output_proj = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        mu = self.mu_proj(hidden[-1])\n",
        "        logvar = self.logvar_proj(hidden[-1])\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, tgt_len):\n",
        "        batch_size = z.size(0)\n",
        "        decoder_input = torch.zeros(batch_size, 1, self.output_size, device=z.device)\n",
        "        z_expanded = z.unsqueeze(1)\n",
        "\n",
        "        outputs = []\n",
        "        hidden = None\n",
        "\n",
        "        for t in range(tgt_len):\n",
        "            decoder_input_with_z = torch.cat([decoder_input, z_expanded], dim=-1)\n",
        "            decoder_output, hidden = self.decoder(decoder_input_with_z, hidden)\n",
        "            output = self.output_proj(decoder_output)\n",
        "            outputs.append(output)\n",
        "            decoder_input = output\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def forward(self, x, tgt_len):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z, tgt_len)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "print(\"Models defined successfully!\")\n"
      ],
      "metadata": {
        "id": "Hr7TBlodo1z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Train Models"
      ],
      "metadata": {
        "id": "ej9KVkKSo5Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_dataset = LearningBehaviorDataset(data['X_train'], data['y_train'])\n",
        "val_dataset = LearningBehaviorDataset(data['X_val'], data['y_val'])\n",
        "test_dataset = LearningBehaviorDataset(data['X_test'], data['y_test'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"Data loaders ready: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")\n",
        "\n",
        "# Train LSTM\n",
        "print(\"\\nTraining LSTM...\")\n",
        "lstm_model = Seq2SeqLSTM(input_size=4, hidden_size=64, output_size=1).to(device)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    lstm_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = lstm_model(X_batch, 2)  # 2 weeks output\n",
        "        loss = F.mse_loss(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"LSTM Epoch {epoch+1}/20: Loss = {total_loss/len(train_loader):.6f}\")\n",
        "\n",
        "print(\"LSTM training completed!\")\n",
        "\n",
        "# Train VAE\n",
        "print(\"\\nTraining VAE...\")\n",
        "vae_model = Seq2SeqVAE(input_size=4, hidden_size=64, latent_dim=16, output_size=1).to(device)\n",
        "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    vae_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = vae_model(X_batch, 2)\n",
        "\n",
        "        # VAE loss = MSE + KL divergence\n",
        "        mse_loss = F.mse_loss(recon, y_batch)\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / y_batch.numel()\n",
        "        loss = mse_loss + kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"VAE Epoch {epoch+1}/20: Loss = {total_loss/len(train_loader):.6f}\")\n",
        "\n",
        "print(\"VAE training completed!\")\n"
      ],
      "metadata": {
        "id": "oPOBZGF9o44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Evaluate and Compare"
      ],
      "metadata": {
        "id": "TU_tcqYPo9lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cell 7: Evaluate and Compare with Detailed Analysis\"\"\"\n",
        "\n",
        "def evaluate_models_detailed():\n",
        "    lstm_model.eval()\n",
        "    vae_model.eval()\n",
        "\n",
        "    lstm_predictions = []\n",
        "    vae_predictions = []\n",
        "    vae_samples = []\n",
        "    targets = []\n",
        "\n",
        "    print(\"Evaluating models...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            lstm_pred = lstm_model(X_batch, 2)\n",
        "            lstm_predictions.append(lstm_pred.cpu().numpy())\n",
        "\n",
        "            vae_pred, _, _ = vae_model(X_batch, 2)\n",
        "            vae_predictions.append(vae_pred.cpu().numpy())\n",
        "\n",
        "            mu, logvar = vae_model.encode(X_batch)\n",
        "            batch_samples = []\n",
        "            for _ in range(20):\n",
        "                z = vae_model.reparameterize(mu, logvar)\n",
        "                sample = vae_model.decode(z, 2)\n",
        "                batch_samples.append(sample.cpu().numpy())\n",
        "            vae_samples.append(np.array(batch_samples))\n",
        "\n",
        "            targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "    lstm_preds = np.concatenate(lstm_predictions)\n",
        "    vae_preds = np.concatenate(vae_predictions)\n",
        "    vae_samples = np.concatenate(vae_samples, axis=1)\n",
        "    all_targets = np.concatenate(targets)\n",
        "\n",
        "    # Calculate per-sample MSE\n",
        "    lstm_mse_per_sample = np.mean((lstm_preds - all_targets) ** 2, axis=(1, 2))\n",
        "    vae_mse_per_sample = np.mean((vae_preds - all_targets) ** 2, axis=(1, 2))\n",
        "\n",
        "    # VAE Best-of-N per sample\n",
        "    vae_best_mse_per_sample = []\n",
        "    for i in range(len(all_targets)):\n",
        "        sample_mses = []\n",
        "        for j in range(20):\n",
        "            mse = np.mean((vae_samples[j, i] - all_targets[i]) ** 2)\n",
        "            sample_mses.append(mse)\n",
        "        vae_best_mse_per_sample.append(min(sample_mses))\n",
        "    vae_best_mse_per_sample = np.array(vae_best_mse_per_sample)\n",
        "\n",
        "    # Overall metrics\n",
        "    lstm_mse = lstm_mse_per_sample.mean()\n",
        "    vae_mse = vae_mse_per_sample.mean()\n",
        "    best_of_n_mse = vae_best_mse_per_sample.mean()\n",
        "\n",
        "    diversity = np.std(vae_samples, axis=0).mean()\n",
        "\n",
        "    lower = np.percentile(vae_samples, 2.5, axis=0)\n",
        "    upper = np.percentile(vae_samples, 97.5, axis=0)\n",
        "    coverage = np.mean((all_targets >= lower) & (all_targets <= upper))\n",
        "\n",
        "    # ===== NEW: Top-5 Regressed Cases (VAE best >> LSTM) =====\n",
        "    improvement = lstm_mse_per_sample - vae_best_mse_per_sample\n",
        "    top5_indices = np.argsort(-improvement)[:5]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"=== Top-5 Regressed (VAE best >> LSTM) ===\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    top5_df = pd.DataFrame({\n",
        "        'idx': top5_indices,\n",
        "        'LSTM_MSE': lstm_mse_per_sample[top5_indices],\n",
        "        'VAE_best_MSE': vae_best_mse_per_sample[top5_indices],\n",
        "        'Œî(LSTM-VAE)': improvement[top5_indices],\n",
        "        'y_true': [all_targets[i].flatten().tolist() for i in top5_indices],\n",
        "        'y_LSTM': [lstm_preds[i].flatten().tolist() for i in top5_indices],\n",
        "        'y_VAE_best': [vae_samples[:, i].mean(axis=0).flatten().tolist() for i in top5_indices],\n",
        "        'Diversity_std': [np.std(vae_samples[:, i]) for i in top5_indices]\n",
        "    })\n",
        "\n",
        "    print(top5_df.to_string(index=True))\n",
        "\n",
        "    # ===== NEW: Win-rate by Improvement Bucket =====\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"=== Win-rate by Improvement bucket (Œî = LSTM MSE - VAE best MSE) ===\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    buckets = [\n",
        "        ('VAEÂ∑Æ>1000', lambda x: x < -1000),\n",
        "        ('VAEÂ∑Æ200~1000', lambda x: (x >= -1000) & (x < -200)),\n",
        "        ('VAEÂ∑Æ50~200', lambda x: (x >= -200) & (x < -50)),\n",
        "        ('VAEÂ∑Æ10~50', lambda x: (x >= -50) & (x < -10)),\n",
        "        ('VAEÁï•Â∑Æ<10', lambda x: (x >= -10) & (x < 0)),\n",
        "        ('Âπ≥Êâã¬±10', lambda x: (x >= 0) & (x < 10)),\n",
        "        ('VAEÁï•Âãù10~50', lambda x: (x >= 10) & (x < 50)),\n",
        "        ('VAEÂãù50~200', lambda x: (x >= 50) & (x < 200)),\n",
        "        ('VAEÂ§ßÂãù200~1000', lambda x: (x >= 200) & (x < 1000)),\n",
        "        ('VAE>>Â§ßÂãù>1000', lambda x: x >= 1000)\n",
        "    ]\n",
        "\n",
        "    bucket_stats = []\n",
        "    for bucket_name, condition in buckets:\n",
        "        mask = condition(improvement)\n",
        "        count = np.sum(mask)\n",
        "        ratio = count / len(improvement) if len(improvement) > 0 else 0\n",
        "        bucket_stats.append({\n",
        "            'Improvement bucket': bucket_name,\n",
        "            'count': count,\n",
        "            'ratio': f\"{ratio:.4f}\"\n",
        "        })\n",
        "\n",
        "    bucket_df = pd.DataFrame(bucket_stats)\n",
        "    print(bucket_df.to_string(index=True))\n",
        "\n",
        "    # ===== NEW: Final =====\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"=========== Ë©ï‰º∞ÁµêÊûú (ÂéüÂßãÂ∞∫Â∫¶) ===========\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Denormalize MSE to original scale\n",
        "    y_std = data['norm_stats']['y_std']\n",
        "    lstm_mse_original = lstm_mse * (y_std ** 2)\n",
        "    vae_mse_original = vae_mse * (y_std ** 2)\n",
        "    best_of_n_original = best_of_n_mse * (y_std ** 2)\n",
        "\n",
        "    # Calculate tau (median MSE for LSTM)\n",
        "    tau = np.median(lstm_mse_per_sample) * (y_std ** 2)\n",
        "    n_minus_28 = len(lstm_mse_per_sample) - 28\n",
        "\n",
        "    print(f\"LSTM    MSE (Êï¥È´î)        : {lstm_mse_original:.4f}\")\n",
        "    print(f\"VAE     Best-of-N MSE     : {vae_mse_original:.4f}     (N=20)\")\n",
        "    print(f\"VAE     Diversity (std)   : {diversity:.4f}\")\n",
        "    print(f\"VAE     Coverage (ÊØî‰æã)    : {coverage:.4f}     (ÈñÄÊ™ª tau = LSTM ÊØèÂàó MSE ‰∏≠‰ΩçÊï∏ = {tau:.4f})\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'lstm_mse': lstm_mse,\n",
        "        'vae_mse': vae_mse,\n",
        "        'vae_best_of_n_mse': best_of_n_mse,\n",
        "        'vae_diversity': diversity,\n",
        "        'vae_coverage': coverage,\n",
        "        'top5_df': top5_df,\n",
        "        'bucket_df': bucket_df\n",
        "    }\n",
        "\n",
        "results = evaluate_models_detailed()"
      ],
      "metadata": {
        "id": "OOGN3sCYo_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8: Create Visualizations"
      ],
      "metadata": {
        "id": "RHjvmhj_pAuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# MSE Comparison\n",
        "axes[0,0].bar(['LSTM', 'VAE'], [results['lstm_mse'], results['vae_mse']], color=['blue', 'red'])\n",
        "axes[0,0].set_title('MSE Comparison')\n",
        "axes[0,0].set_ylabel('MSE')\n",
        "\n",
        "# VAE Best-of-N vs Mean\n",
        "axes[0,1].bar(['VAE Mean', 'VAE Best-of-N'], [results['vae_mse'], results['vae_best_of_n_mse']], color=['red', 'green'])\n",
        "axes[0,1].set_title('VAE: Mean vs Best-of-N')\n",
        "axes[0,1].set_ylabel('MSE')\n",
        "\n",
        "# VAE Diversity\n",
        "axes[1,0].bar(['VAE'], [results['vae_diversity']], color='orange')\n",
        "axes[1,0].set_title('VAE Diversity')\n",
        "axes[1,0].set_ylabel('Standard Deviation')\n",
        "\n",
        "# VAE Coverage\n",
        "axes[1,1].bar(['VAE'], [results['vae_coverage']], color='purple')\n",
        "axes[1,1].set_title('VAE Coverage (95% CI)')\n",
        "axes[1,1].set_ylabel('Proportion')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "summary_data = {\n",
        "    'Metric': ['MSE', 'Best-of-N MSE', 'Diversity', 'Coverage'],\n",
        "    'LSTM': [f\"{results['lstm_mse']:.6f}\", 'N/A', '0 (deterministic)', 'N/A'],\n",
        "    'VAE': [\n",
        "        f\"{results['vae_mse']:.6f}\",\n",
        "        f\"{results['vae_best_of_n_mse']:.6f}\",\n",
        "        f\"{results['vae_diversity']:.6f}\",\n",
        "        f\"{results['vae_coverage']:.3f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nMODEL COMPARISON TABLE:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüéâ Experiment completed successfully!\")\n",
        "print(\"üìä Use the results above for your assignment report.\")\n"
      ],
      "metadata": {
        "id": "RUzH_NMKpCgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}