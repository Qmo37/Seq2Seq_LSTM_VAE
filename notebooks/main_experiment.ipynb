{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq LSTM vs VAE for Learning Behavior Prediction\n",
        "\n",
        "This notebook compares two sequence generation models on the Open University Learning Analytics Dataset (OULAD):\n",
        "- **Seq2Seq LSTM**: Deterministic single-path prediction\n",
        "- **Seq2Seq VAE**: Probabilistic multi-path generation\n",
        "\n",
        "## 🎯 Objectives\n",
        "1. Train both models on the same dataset\n",
        "2. Compare single-path prediction accuracy (MSE)\n",
        "3. Analyze diversity and coverage of VAE generations\n",
        "4. Visualize and interpret results\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Before You Start\n",
        "\n",
        "**IMPORTANT**: You need to upload the OULAD dataset files:\n",
        "- `studentInfo.csv`\n",
        "- `studentVle.csv`\n",
        "- `studentAssessment.csv`\n",
        "\n",
        "**Download from**: [Kaggle OULAD Dataset](https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad)\n",
        "\n",
        "**🚀 Quick Start**: Run all cells in order!"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages and check environment\n",
        "!pip install torch torchvision torchaudio matplotlib seaborn pandas numpy scikit-learn tqdm -q\n",
        "\n",
        "import os, sys, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "print(f\"🌐 Running in Colab: {IN_COLAB}\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🖥️  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"🔋 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n✅ Environment setup complete!\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configuration - DO NOT CHANGE (Assignment Requirements)\n",
        "CONFIG = {\n",
        "    'input_weeks': 4,        # Fixed: 4 weeks input\n",
        "    'output_weeks': 2,       # Fixed: 2 weeks output\n",
        "    'batch_size': 128,       # Fixed: batch size\n",
        "    'learning_rate': 1e-3,   # Fixed: Adam learning rate\n",
        "    'random_seed': 42,       # Fixed: reproducibility\n",
        "    \n",
        "    # Adjustable hyperparameters\n",
        "    'epochs': 20,\n",
        "    'hidden_size': 64,\n",
        "    'latent_dim': 16,\n",
        "    'beta': 1.0,             # VAE KL weight\n",
        "    'n_samples': 20          # Number of VAE samples\n",
        "}\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(CONFIG['random_seed'])\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "os.makedirs('results/figures', exist_ok=True)\n",
        "os.makedirs('results/checkpoints', exist_ok=True)\n",
        "\n",
        "print(\"📊 Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\n✅ Libraries imported and configuration set!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Upload and Loading"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data upload helper for Colab\n",
        "def upload_data_files():\n",
        "    \"\"\"Helper function to upload data files in Colab.\"\"\"\n",
        "    if IN_COLAB:\n",
        "        from google.colab import files\n",
        "        print(\"📤 Upload your OULAD CSV files:\")\n",
        "        print(\"   Expected files: studentInfo.csv, studentVle.csv, studentAssessment.csv\")\n",
        "        print(\"\\n⬇️ Click 'Choose Files' below and select all 3 CSV files:\")\n",
        "        \n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        # Move files to data/raw/\n",
        "        for filename in uploaded.keys():\n",
        "            if filename.endswith('.csv'):\n",
        "                os.rename(filename, f'data/raw/{filename}')\n",
        "                print(f\"✅ Moved {filename} to data/raw/\")\n",
        "        \n",
        "        print(\"\\n🎉 Upload complete! You can now run the next cells.\")\n",
        "    else:\n",
        "        print(\"📁 Not in Colab - please ensure CSV files are in data/raw/ folder\")\n",
        "\n",
        "def check_data_files():\n",
        "    \"\"\"Check if all required data files are present.\"\"\"\n",
        "    required_files = ['studentInfo.csv', 'studentVle.csv', 'studentAssessment.csv']\n",
        "    missing = [f for f in required_files if not os.path.exists(f'data/raw/{f}')]\n",
        "    \n",
        "    if missing:\n",
        "        print(f\"❌ Missing files: {missing}\")\n",
        "        print(\"\\n🔧 Uncomment and run the line below to upload files:\")\n",
        "        print(\"# upload_data_files()\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ All data files found!\")\n",
        "        for f in required_files:\n",
        "            size = os.path.getsize(f'data/raw/{f}') / (1024*1024)\n",
        "            print(f\"   📄 {f}: {size:.1f} MB\")\n",
        "        return True\n",
        "\n",
        "# Check for data files\n",
        "data_ready = check_data_files()\n",
        "\n",
        "# Uncomment the next line if you need to upload files\n",
        "# upload_data_files()"
      ],
      "metadata": {
        "id": "data_upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_oulad_data(data_path='data/raw'):\n",
        "    \"\"\"Load OULAD dataset with robust error handling.\"\"\"\n",
        "    print(\"📂 Loading OULAD dataset...\")\n",
        "    \n",
        "    try:\n",
        "        # Load CSV files\n",
        "        student_info = pd.read_csv(f\"{data_path}/studentInfo.csv\")\n",
        "        student_vle = pd.read_csv(f\"{data_path}/studentVle.csv\")\n",
        "        student_assessment = pd.read_csv(f\"{data_path}/studentAssessment.csv\")\n",
        "        \n",
        "        print(f\"✅ studentInfo: {student_info.shape}\")\n",
        "        print(f\"✅ studentVle: {student_vle.shape}\")\n",
        "        print(f\"✅ studentAssessment: {student_assessment.shape}\")\n",
        "        \n",
        "        # Dataset info\n",
        "        print(f\"\\n📊 Dataset overview:\")\n",
        "        print(f\"   👥 Students: {student_info['id_student'].nunique():,}\")\n",
        "        print(f\"   🖱️ VLE clicks: {len(student_vle):,}\")\n",
        "        print(f\"   📝 Assessments: {len(student_assessment):,}\")\n",
        "        \n",
        "        # Check columns\n",
        "        print(f\"\\n🔍 Available columns:\")\n",
        "        print(f\"   VLE: {list(student_vle.columns)}\")\n",
        "        print(f\"   Assessment: {list(student_assessment.columns)}\")\n",
        "        \n",
        "        return student_info, student_vle, student_assessment\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading data: {str(e)}\")\n",
        "        print(\"\\n💡 Make sure you've uploaded the CSV files first!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load data if files are available\n",
        "if check_data_files():\n",
        "    student_info, student_vle, student_assessment = load_oulad_data()\n",
        "else:\n",
        "    print(\"⏭️ Skipping data loading - upload files first\")\n",
        "    student_info, student_vle, student_assessment = None, None, None"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "preprocessing_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_weekly_features(student_info, student_vle, student_assessment, max_weeks=30):\n",
        "    \"\"\"Aggregate student data into weekly features with robust column handling.\"\"\"\n",
        "    if student_info is None:\n",
        "        print(\"❌ No data to process\")\n",
        "        return None\n",
        "    \n",
        "    print(\"🔧 Aggregating weekly features...\")\n",
        "    \n",
        "    # Handle VLE date column - check multiple possible names\n",
        "    vle_date_col = None\n",
        "    possible_vle_date_cols = ['date', 'day', 'date_registration']\n",
        "    for col in possible_vle_date_cols:\n",
        "        if col in student_vle.columns:\n",
        "            vle_date_col = col\n",
        "            break\n",
        "    \n",
        "    if vle_date_col:\n",
        "        student_vle[\"week\"] = student_vle[vle_date_col] // 7\n",
        "        print(f\"✅ Using '{vle_date_col}' for VLE dates\")\n",
        "    else:\n",
        "        raise KeyError(f\"No date column found in VLE data. Available: {list(student_vle.columns)}\")\n",
        "    \n",
        "    # Handle assessment date column - check multiple possible names\n",
        "    assess_date_col = None\n",
        "    possible_assess_date_cols = [\"date_submitted\", \"date\", \"submission_date\", \"date_submit\", \"day\"]\n",
        "    for col in possible_assess_date_cols:\n",
        "        if col in student_assessment.columns:\n",
        "            assess_date_col = col\n",
        "            break\n",
        "    \n",
        "    if assess_date_col:\n",
        "        student_assessment[\"week\"] = student_assessment[assess_date_col] // 7\n",
        "        print(f\"✅ Using '{assess_date_col}' for assessment dates\")\n",
        "    else:\n",
        "        print(\"⚠️ No date column found in assessments - using week 0\")\n",
        "        student_assessment[\"week\"] = 0\n",
        "    \n",
        "    # Handle click columns - check multiple possible names\n",
        "    click_col = None\n",
        "    possible_click_cols = [\"sum_click\", \"clicks\", \"click_count\", \"num_clicks\"]\n",
        "    for col in possible_click_cols:\n",
        "        if col in student_vle.columns:\n",
        "            click_col = col\n",
        "            break\n",
        "    \n",
        "    if not click_col:\n",
        "        raise KeyError(f\"No click column found in VLE data. Available: {list(student_vle.columns)}\")\n",
        "    \n",
        "    print(f\"✅ Using '{click_col}' for click data\")\n",
        "    \n",
        "    # Aggregate clicks per student per week\n",
        "    clicks_df = student_vle.groupby([\"id_student\", \"week\"])[click_col].sum().reset_index()\n",
        "    clicks_df.columns = [\"id_student\", \"week\", \"clicks\"]\n",
        "    \n",
        "    # Aggregate assessments - handle score column variations\n",
        "    score_col = None\n",
        "    possible_score_cols = ['score', 'final_result', 'grade', 'points']\n",
        "    for col in possible_score_cols:\n",
        "        if col in student_assessment.columns:\n",
        "            score_col = col\n",
        "            break\n",
        "    \n",
        "    if score_col and len(student_assessment) > 0:\n",
        "        # Convert non-numeric scores to numeric if needed\n",
        "        if student_assessment[score_col].dtype == 'object':\n",
        "            print(f\"⚠️ Converting {score_col} to numeric values\")\n",
        "            score_mapping = {\n",
        "                'Pass': 70, 'Fail': 30, 'Distinction': 85, 'Withdrawn': 0,\n",
        "                'A': 90, 'B': 80, 'C': 70, 'D': 60, 'F': 0\n",
        "            }\n",
        "            student_assessment[score_col] = student_assessment[score_col].map(\n",
        "                lambda x: score_mapping.get(x, pd.to_numeric(x, errors='coerce'))\n",
        "            )\n",
        "        \n",
        "        submit_df = student_assessment.groupby([\"id_student\", \"week\"]).agg({\n",
        "            score_col: [\"count\", \"mean\"]\n",
        "        }).reset_index()\n",
        "        submit_df.columns = [\"id_student\", \"week\", \"submit_cnt\", \"avg_score\"]\n",
        "        print(f\"✅ Processed assessment scores using '{score_col}'\")\n",
        "    else:\n",
        "        submit_df = pd.DataFrame(columns=[\"id_student\", \"week\", \"submit_cnt\", \"avg_score\"])\n",
        "        print(\"⚠️ No assessment scores found - using dummy data\")\n",
        "    \n",
        "    # Create complete week grid\n",
        "    student_ids = student_info[\"id_student\"].unique()\n",
        "    print(f\"📅 Creating {max_weeks} weeks for {len(student_ids)} students...\")\n",
        "    \n",
        "    all_weeks = []\n",
        "    for student_id in student_ids:\n",
        "        for week in range(max_weeks):\n",
        "            all_weeks.append({\"id_student\": student_id, \"week\": week})\n",
        "    \n",
        "    df = pd.DataFrame(all_weeks)\n",
        "    \n",
        "    # Merge features\n",
        "    df = df.merge(clicks_df, on=[\"id_student\", \"week\"], how=\"left\")\n",
        "    if not submit_df.empty:\n",
        "        df = df.merge(submit_df, on=[\"id_student\", \"week\"], how=\"left\")\n",
        "    else:\n",
        "        df[\"submit_cnt\"] = 0\n",
        "        df[\"avg_score\"] = 0\n",
        "    \n",
        "    # Fill missing values\n",
        "    df[\"clicks\"] = df[\"clicks\"].fillna(0)\n",
        "    df[\"submit_cnt\"] = df[\"submit_cnt\"].fillna(0)\n",
        "    df[\"avg_score\"] = df[\"avg_score\"].fillna(0)\n",
        "    \n",
        "    # Sort and create derived features\n",
        "    df = df.sort_values([\"id_student\", \"week\"]).reset_index(drop=True)\n",
        "    df[\"has_submit\"] = (df[\"submit_cnt\"] > 0).astype(int)\n",
        "    \n",
        "    # Calculate cumulative average score per student\n",
        "    df[\"avg_score_sofar\"] = df.groupby(\"id_student\")[\"avg_score\"].transform(\n",
        "        lambda x: x.expanding().mean()\n",
        "    )\n",
        "    \n",
        "    # Calculate first-order difference of clicks\n",
        "    df[\"clicks_diff1\"] = df.groupby(\"id_student\")[\"clicks\"].diff().fillna(0)\n",
        "    \n",
        "    print(f\"✅ Weekly features created: {df.shape}\")\n",
        "    print(f\"   📊 Features: clicks, has_submit, avg_score_sofar, clicks_diff1\")\n",
        "    \n",
        "    # Show some statistics\n",
        "    print(f\"\\n📈 Feature statistics:\")\n",
        "    print(f\"   Clicks: mean={df['clicks'].mean():.1f}, max={df['clicks'].max()}\")\n",
        "    print(f\"   Submissions: {df['has_submit'].sum()} total submissions\")\n",
        "    print(f\"   Average score: {df['avg_score'].mean():.1f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Process data if available\n",
        "if student_info is not None:\n",
        "    weekly_df = aggregate_weekly_features(student_info, student_vle, student_assessment)\n",
        "else:\n",
        "    print(\"⏭️ Skipping feature aggregation - upload data files first\")\n",
        "    weekly_df = None"
      ],
      "metadata": {
        "id": "aggregate_features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, input_weeks=4, output_weeks=2):\n",
        "    \"\"\"Create input-output sequences for seq2seq learning.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"❌ No data for sequence creation\")\n",
        "        return None, None, None\n",
        "    \n",
        "    feature_cols = [\"clicks\", \"has_submit\", \"avg_score_sofar\", \"clicks_diff1\"]\n",
        "    print(f\"🔧 Creating sequences: {input_weeks} weeks → {output_weeks} weeks\")\n",
        "    print(f\"   📊 Input features: {feature_cols}\")\n",
        "    print(f\"   🎯 Output: clicks only\")\n",
        "    \n",
        "    X_list, y_list, student_id_list = [], [], []\n",
        "    \n",
        "    for student_id, group in tqdm(df.groupby(\"id_student\"), desc=\"Processing students\"):\n",
        "        group = group.sort_values(\"week\").reset_index(drop=True)\n",
        "        \n",
        "        if len(group) < input_weeks + output_weeks:\n",
        "            continue\n",
        "        \n",
        "        # Create sliding windows\n",
        "        for i in range(len(group) - input_weeks - output_weeks + 1):\n",
        "            X_window = group.iloc[i : i + input_weeks][feature_cols].values\n",
        "            y_window = group.iloc[i + input_weeks : i + input_weeks + output_weeks][[\"clicks\"]].values\n",
        "            \n",
        "            X_list.append(X_window)\n",
        "            y_list.append(y_window)\n",
        "            student_id_list.append(student_id)\n",
        "    \n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=np.float32)\n",
        "    student_ids = np.array(student_id_list)\n",
        "    \n",
        "    print(f\"✅ Created {len(X):,} sequences\")\n",
        "    print(f\"   📐 Input shape: {X.shape} (batch, time, features)\")\n",
        "    print(f\"   📐 Output shape: {y.shape} (batch, time, 1)\")\n",
        "    \n",
        "    return X, y, student_ids\n",
        "\n",
        "def split_and_normalize_data(X, y, student_ids, test_size=0.2, val_size=0.1, random_seed=42):\n",
        "    \"\"\"Split by students and normalize to avoid data leakage.\"\"\"\n",
        "    if X is None:\n",
        "        return None\n",
        "    \n",
        "    print(\"🔧 Splitting data by student ID (avoiding data leakage)...\")\n",
        "    \n",
        "    # Split by student to prevent data leakage\n",
        "    unique_students = np.unique(student_ids)\n",
        "    train_students, test_students = train_test_split(\n",
        "        unique_students, test_size=test_size, random_state=random_seed\n",
        "    )\n",
        "    train_students, val_students = train_test_split(\n",
        "        train_students, test_size=val_size, random_state=random_seed\n",
        "    )\n",
        "    \n",
        "    # Create masks\n",
        "    train_mask = np.isin(student_ids, train_students)\n",
        "    val_mask = np.isin(student_ids, val_students)\n",
        "    test_mask = np.isin(student_ids, test_students)\n",
        "    \n",
        "    # Split data\n",
        "    X_train, y_train = X[train_mask], y[train_mask]\n",
        "    X_val, y_val = X[val_mask], y[val_mask]\n",
        "    X_test, y_test = X[test_mask], y[test_mask]\n",
        "    \n",
        "    print(f\"✅ Data split:\")\n",
        "    print(f\"   🚂 Train: {len(X_train):,} sequences ({len(train_students)} students)\")\n",
        "    print(f\"   🔍 Val: {len(X_val):,} sequences ({len(val_students)} students)\")\n",
        "    print(f\"   🧪 Test: {len(X_test):,} sequences ({len(test_students)} students)\")\n",
        "    \n",
        "    # Normalize using only training statistics\n",
        "    print(\"\\n🔧 Normalizing features using training set statistics...\")\n",
        "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
        "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std() + 1e-8\n",
        "    \n",
        "    # Apply normalization\n",
        "    X_train_norm = (X_train - X_mean) / X_std\n",
        "    X_val_norm = (X_val - X_mean) / X_std\n",
        "    X_test_norm = (X_test - X_mean) / X_std\n",
        "    \n",
        "    y_train_norm = (y_train - y_mean) / y_std\n",
        "    y_val_norm = (y_val - y_mean) / y_std\n",
        "    y_test_norm = (y_test - y_mean) / y_std\n",
        "    \n",
        "    norm_stats = {\n",
        "        \"X_mean\": X_mean, \"X_std\": X_std,\n",
        "        \"y_mean\": y_mean, \"y_std\": y_std\n",
        "    }\n",
        "    \n",
        "    print(f\"✅ Normalization complete\")\n",
        "    print(f\"   📊 X normalization: mean≈0, std≈1\")\n",
        "    print(f\"   📊 y stats: mean={y_mean:.3f}, std={y_std:.3f}\")\n",
        "    \n",
        "    return {\n",
        "        \"X_train\": X_train_norm, \"y_train\": y_train_norm,\n",
        "        \"X_val\": X_val_norm, \"y_val\": y_val_norm,\n",
        "        \"X_test\": X_test_norm, \"y_test\": y_test_norm,\n",
        "        \"norm_stats\": norm_stats\n",
        "    }\n",
        "\n",
        "# Create sequences and split data\n",
        "if weekly_df is not None:\n",
        "    X, y, student_ids = create_sequences(weekly_df, \n",
        "                                        input_weeks=CONFIG['input_
